{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Just1got/Hello_python/blob/main/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8_%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%B0%D1%86%D0%B8%D0%B8_v2%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FWR90xxpSl"
      },
      "source": [
        "# Функции активации\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сегодня обсудим:\n",
        "1. Какие есть функции активации нейронов (Sigmoid, ReLU, ELU, Leaky ReLU...)\n",
        "2. Плюсы каждой функции активации\n",
        "3. Минусы каждой функции активации\n",
        "4. Как написать свою функцию активации"
      ],
      "metadata": {
        "id": "lsgGTfpbH4-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Для чего нужны?"
      ],
      "metadata": {
        "id": "Pr1-IHkeQYpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1gF12si7S9po9bBfJ9Q7bfmM5v55vi3tE'>\n",
        "\n",
        "Наш нейрон из себя представляет взвешенную сумму:\n",
        "\n",
        "$$y = \\sum{XW} + bias$$\n",
        "\n",
        "И значение $y$ может изменяться от $-∞$ до $+∞$. И значит *всегда* этот нейрон будет пропускать через себя сигнал. У него **нет границы** активации.\n",
        "\n",
        "А если вспомним биологический нейрон, то чтобы нейрон активировался (пропустил информацию дальше), нужно поднакопить необходимый сигнал. А значит, раз искусственный нейрон основан на биологическом нейроне, то и **активировать** его мы будем должным образом.\n",
        "\n",
        "И поможет нам в этом фукнция активации. Ее задача заключается в том, чтобы проверять, является ли нейрон активированным, или его можно проигнорировать."
      ],
      "metadata": {
        "id": "fGVPLAmJXidO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Основная задача на сегодня"
      ],
      "metadata": {
        "id": "8XAJ86DF6BZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(X_train, y_train_labels), (X_test, y_test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "I55u4X-J5kzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top',\n",
        "               'Trouser',\n",
        "               'Pullover',\n",
        "               'Dress',\n",
        "               'Coat',\n",
        "               'Sandal',\n",
        "               'Shirt',\n",
        "               'Sneaker',\n",
        "               'Bag',\n",
        "               'Ankle boot']"
      ],
      "metadata": {
        "id": "rs1KRcox5-j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_labels"
      ],
      "metadata": {
        "id": "Nn44i7HiKvh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train_labels)\n",
        "y_test = to_categorical(y_test_labels)"
      ],
      "metadata": {
        "id": "wIKT6m7a5owt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "ECGVyWpsKyvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(X_train[0], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z99r-_Nr5uIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нормализуем данные, из диапазона от 0 до 255 переводим в диапазон от 0 до 1."
      ],
      "metadata": {
        "id": "SlP4NjHJ50CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "metadata": {
        "id": "aV8OSFXO55QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Линейная функция\n",
        "\n",
        "$$A(x) = x$$\n",
        "\n",
        "Линейная функция представляет собой прямую линию и она пропорциональна входу."
      ],
      "metadata": {
        "id": "4k6swcWLfRzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.activations import linear\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = linear(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('linear')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "W5p81P4bg51j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Производная такой функции по $x$ равняется:\n",
        "\n",
        "$$\\frac{∂A}{∂x} = 1$$"
      ],
      "metadata": {
        "id": "txcNIApJiF3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "y = linear(x)\n",
        "\n",
        "grad_y = np.ones_like(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.plot(x, grad_y, c='g', label='grad')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('linear')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "yP6VaGN9iC3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Первая проблема\n",
        "\n",
        "Для этой функции производная всегда одинакова в любой точке.\n",
        "\n",
        "А это значит, что градиент никак не связан с входом ($x$). Градиент - постоянный вектор, а спуск идет по **постоянному** градиенту. Если есть ошибочное предсказание, то изменения, сделанные обратным распространением ошибки, тоже постоянны вне зависимости от входа."
      ],
      "metadata": {
        "id": "mfAvJISrg6n7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы это проверить, создадим обучающие данные, будем обучать нейронную сеть на воспроизведение входа."
      ],
      "metadata": {
        "id": "d9UDd0y8TbJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [3], [2]])\n",
        "y = np.array([[1, 3, 2]]).T"
      ],
      "metadata": {
        "id": "aMwK0J81-AJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделаем один линейные слой с линейной активацией и уберем свободный вес."
      ],
      "metadata": {
        "id": "AzQVJ42byNkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(1, input_shape=(1,), activation='linear', use_bias=False),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "c6UWgSx29ohw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_weights()"
      ],
      "metadata": {
        "id": "RVyCuttxsvQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем следить за градиентами на очень низком уровне, для этого нам потребуется объект из tensorflow `GradientTape`, который записывает градиенты."
      ],
      "metadata": {
        "id": "p7amuc24yVtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.losses import MSE\n",
        "\n",
        "\n",
        "for i in range(3):     \n",
        "    with tf.GradientTape() as tape: \n",
        "        pred = model(X[i])\n",
        "        print('Prediction is', pred.numpy(), 'True is', y[i])\n",
        "\n",
        "        loss_value = MSE(y[i], pred)\n",
        "        print('Loss is', loss_value.numpy())\n",
        "\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights) \n",
        "        print('Grad are', grads / loss_value)\n",
        "        print('_' * 40)"
      ],
      "metadata": {
        "id": "MWW4M4PYmXtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Вторая проблема\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1pLd9DW7HxfcxoXyS3mRaENS8ugpxeSou' width=800>\n",
        "\n",
        "\n",
        "Раз каждый слой активируется линейной функцией, то в следующий слой пойдет линейная связь от входа. В другом слое получаем линейную комбинацию, прогоняем через линейную активацию и дальше.\n",
        "\n",
        "Нет разницы, сколько у нас слоев. Ведь финальная функция активации в последнем слое будет просто линейной функцией от входных параметров на первом слое.\n",
        "\n",
        "Значит, любое количество слоев можно заменить одним. Больше нет смысла ставить подряд слои с линейной активацией.\n",
        "\n"
      ],
      "metadata": {
        "id": "nr6rBWv4hAU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сигмоида\n",
        "\n",
        "Сигмоида - одна из самых частых активационных функций в нейросетях.\n",
        "\n",
        "\n",
        "$$A(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "\n",
        "**Преимущества:**\n",
        "1. Сигмоида — **нелинейная функция**, а комбинация таких функций производит тоже нелинейную функцию. А значит проблема линейной функция здесь неактуальна, мы можем обучать подряд **много слоев** и это будет иметь смысл.\n",
        "2. В диапазоне значений сигнала от -2.5 до 2.5 значения активации меняются очень быстро. А значит, что любое изменение значения сигнала в этой области повлечет существенное изменение значения активации. \n",
        "3. Сигмоида идеально подходит для задач **бинарной классификации**. Она стремится привести значения к одной из сторон кривой, по итогу получаем четкие границы предсказания\n",
        "4. У сигмоиды фиксированный диапазон значений функции — [0,1], а линейная функция обладает диапазоном ($-∞$, $+∞$). А значит в случае с сигмоидой **не будет ошибок** с большими значениями активаций\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sesjAUrq12LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.activations import sigmoid\n",
        "\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = sigmoid(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('sigmoid')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "N_5FTMfa2FIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Производная сигмоиды\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1 + exp(-z)}$$\n",
        "\n",
        "$$\\frac{d\\sigma(z)}{dz} = -\\frac{1}{(1 + exp(-z))^2}(-exp(-z)) = \\frac{exp(-z)}{(1 + exp(-z))^2} \\text{(*)} = \\frac{1}{1 + exp(-z)}(1-\\frac{1}{1 + exp(-z)}) = \\sigma(1-\\sigma)$$\n",
        "\n",
        "$ (*)\\frac{exp(-z) +1-1}{(1 + exp(-z))^2} = \\frac{1 + exp(-z) -1}{(1 + exp(-z))^2}= \\frac{1 + exp(-z)}{(1 + exp(-z))^2} - \\frac{1}{(1 + exp(-z))^2}=$\n",
        "\n",
        "$ = \\frac{1}{(1 + exp(-z))} - \\frac{1}{(1 + exp(-z))}\\frac{1}{(1 + exp(-z))} = \\frac{1}{(1 + exp(-z))}(1- \\frac{1}{(1 + exp(-z))}) $"
      ],
      "metadata": {
        "id": "iDIBOvMq2FIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_grad(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "metadata": {
        "id": "g1BDRq1r2Smt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "y = sigmoid(x)\n",
        "\n",
        "grad_y = sigmoid_grad(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.plot(x, grad_y, c='g', label='grad')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('sigmoid')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "TfTVrYhH2FIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "0.25 ** 5 * 0.001"
      ],
      "metadata": {
        "id": "eP9bKQdaQJwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Проблема\n",
        "\n",
        "При приближении к концам сигмоиды значения активации очень слабо изменяются. Это означает, что градиент в таких областях имеет очень мальнькие значения. А из-за этого рождается проблема **затухающих градиентов**\n",
        "\n",
        "Градиент получается настолько маленьким или же вообще нулевым, что обучение замораживается или же идет очень медленно."
      ],
      "metadata": {
        "id": "4fmXMtLo1_cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Flatten\n",
        "tf.random.set_seed(9)\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='sigmoid'),\n",
        "    Dense(64, activation='sigmoid'),\n",
        "    Dense(32, activation='sigmoid'),\n",
        "    Dense(16, activation='sigmoid'),\n",
        "    Dense(10, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "uExVUIqH6IzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.losses import CategoricalCrossentropy\n",
        "#from keras.optimizers.legacy import optimizer_v2\n",
        "#from keras.optimizer_v2 import gradient_descent\n",
        "\n",
        "optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.01)\n",
        "loss_fn = CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "t6aHGuqV6aUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовим тренировочный датасет\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \n",
        "train_dataset = train_dataset.shuffle(buffer_size=X_train.shape[0]).batch(batch_size) "
      ],
      "metadata": {
        "id": "JGNri8JR-j3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in train_dataset:\n",
        "    print(X.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "_ds5ZHPD-3ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_log = []\n",
        "\n",
        "epochs = 2 \n",
        "\n",
        "for epoch in range(epochs): \n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): \n",
        "        \n",
        "        with tf.GradientTape() as tape: \n",
        "            preds = model(x_batch_train) \n",
        "\n",
        "            loss_value = loss_fn(y_batch_train, preds)\n",
        "\n",
        "            grads = tape.gradient(loss_value, model.trainable_weights) \n",
        "            g_g = []\n",
        "\n",
        "            for g_s in grads:\n",
        "                if len(g_s.numpy().shape) == 2:\n",
        "                    g_g.append(g_s.numpy()[0, 0]) \n",
        "\n",
        "        grad_log.append(g_g)\n",
        "\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}', end='. ')\n",
        "            print(f'Step {step}. Train loss: {loss_value}') "
      ],
      "metadata": {
        "id": "2PLJUul26Qk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_log = np.array(grad_log)\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "plt.title('Grad by layer')\n",
        "plt.xlabel('№ layer')\n",
        "plt.ylabel('grad')\n",
        "plt.grid()\n",
        "plt.plot(np.abs(grad_log[0, :]), label='step 0')\n",
        "plt.plot(np.abs(grad_log[10, :]), label='step 10')\n",
        "plt.plot(np.abs(grad_log[100, :]), label='step 100')\n",
        "plt.plot(np.abs(grad_log[700, :]), label='step 700')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UCvv0Xtm_JJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Тангенс\n",
        "\n",
        "$$tanh(x)={\\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}={\\frac {e^{2x}-1}{e^{2x}+1}} = 2σ(2x) - 1$$"
      ],
      "metadata": {
        "id": "N_7ZTC1bAScL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.activations import tanh\n",
        "\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = tanh(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('tanh')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "SltysuZ9ArvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Производная тангенса\n",
        "\n",
        "$$∂\\tanh(z) = \\frac{1}{ch^2(x)} = \\frac{1}{(\\frac{e^x + e^{-x}}{2})^2}$$\n",
        "\n",
        "$$ch(x) = \\frac{e^x + e^{-x}}{2}$$"
      ],
      "metadata": {
        "id": "ZiH31uVGArvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh_grad(x):\n",
        "    return 1 / (np.exp(x) + np.exp(-x) / 2) ** 2"
      ],
      "metadata": {
        "id": "ZnmMTuSPArvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "y = tanh(x)\n",
        "\n",
        "grad_y = tanh_grad(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.plot(x, grad_y, c='g', label='grad')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('tanh')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "YBS-d2u4ArvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гиперболический тангенс очень похож на сигмоиду, только вот производная у него чуть больше, но все остальные плюсы и минусы остаются.\n",
        "\n",
        "**Плюсы:**\n",
        "1. Тангенс - **нелинейная функция**, можем обучать подряд много слоев и это будет иметь смысл.\n",
        "2. В диапазоне значений сигнала от -2.5 до 2.5 значения активации меняются очень быстро. А значит, что любое изменение значения сигнала в этой области повлечет существенное изменение значения активации.\n",
        "3. Тангенс идеально подходит для задач **бинарной классификации**. Он стремится привести значения к одной из сторон кривой.\n",
        "4. У тангенса фиксированный диапазон значений функции — [-1,1]. А значит не будет ошибок с большими значениями аткиваций\n",
        "\n",
        "\n",
        "**Минусы:**\n",
        "1. При приближении к концам тангенса значения активации очень слабо изменяются. Имеем проблему **затухающих градиентов**."
      ],
      "metadata": {
        "id": "VUgJWIakAQ89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLU\n",
        "\n",
        "\n",
        "$$A(x) = max(0, x)$$\n",
        "\n",
        "Может показаться, что у ReLU все те же самые проблемы, что и у линейной функция. Но на самом деле, ReLU нелинейна. А это значит, что мы можем создавать подряд много слоев. \n",
        "\n",
        "ReLU менее требовательна к вычислительным ресурсам, чем тангенс или сигмоида, поэтому ReLU пользуется популярностью при создании глубоких нейронных сетей."
      ],
      "metadata": {
        "id": "1zzKgTZXSVxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.activations import relu\n",
        "\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = relu(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('ReLU')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "o3wfrXHMRSWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Производная ReLU"
      ],
      "metadata": {
        "id": "_58gl5ucRSWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_grad(x):\n",
        "    return np.where(x >= 0, 1, 0)"
      ],
      "metadata": {
        "id": "V-6ePlamRSWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "y = relu(x)\n",
        "\n",
        "grad_y = relu_grad(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.plot(x, grad_y, c='g', label='grad')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('relu')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "hAY6aIidRSWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим, что проблемы затухающих градиентов больше нет."
      ],
      "metadata": {
        "id": "i8gkdDv9cLLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(9)\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(10, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ItexJeNOcJgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.01)\n",
        "loss_fn = CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "7lfJOtS4cJgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_log = []\n",
        "\n",
        "epochs = 2 \n",
        "\n",
        "for epoch in range(epochs): \n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): \n",
        "        \n",
        "        with tf.GradientTape() as tape: \n",
        "            preds = model(x_batch_train) \n",
        "\n",
        "            loss_value = loss_fn(y_batch_train, preds)\n",
        "\n",
        "            grads = tape.gradient(loss_value, model.trainable_weights) \n",
        "            g_g = []\n",
        "\n",
        "            for g_s in grads:\n",
        "                if len(g_s.numpy().shape) == 2:\n",
        "                    g_g.append(g_s.numpy()[0, 0]) \n",
        "\n",
        "        grad_log.append(g_g)\n",
        "\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}', end='. ')\n",
        "            print(f'Step {step}. Train loss: {loss_value}') "
      ],
      "metadata": {
        "id": "9j-B5BSNcJgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_log = np.array(grad_log)\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "plt.title('Grad by layer')\n",
        "plt.xlabel('№ layer')\n",
        "plt.ylabel('grad')\n",
        "plt.grid()\n",
        "plt.plot(np.abs(grad_log[0, :]), label='step 0')\n",
        "plt.plot(np.abs(grad_log[10, :]), label='step 10')\n",
        "plt.plot(np.abs(grad_log[100, :]), label='step 100')\n",
        "plt.plot(np.abs(grad_log[700, :]), label='step 700')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LpUYMyF4cJgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Проблема\n",
        "\n",
        "\n",
        "Если имеем большую нейронную сеть с огромным количеством нейронов, то использование сигмоиды или гиперболического тангенса повлекут за собой **активацию буквально всех нейронов**. А в идеале хочется, чтобы некоторые нейроны не были активированы, чтобы сеть была легче.\n",
        "\n",
        "ReLu может это сделать. К примеру, если у нас есть нейросеть со случайно инициализированными весами, то 50% активаций равны 0 по ReLU.\n",
        "\n",
        "Но и тут есть проблема. Градиент на этой части равен 0. А значит, веса сети не будут изменяться во время обучения.\n",
        "\n",
        "\n",
        "Но существуют модификации ReLU, которые помогают эту проблему решить.\n",
        "\n"
      ],
      "metadata": {
        "id": "p7glU6SnRaLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1], [3], [2]])\n",
        "y = np.array([[1, 3, 2]]).T"
      ],
      "metadata": {
        "id": "RQG4CMiJbtw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделаем один линейные слой с активацией ReLU и уберем свободный вес."
      ],
      "metadata": {
        "id": "DhwNjm4AbtxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(1, input_shape=(1,), activation='relu', use_bias=False),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Wk2AxC5lbtxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_weights()"
      ],
      "metadata": {
        "id": "2TkIhqenbtxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем следить за градиентами на очень низком уровне."
      ],
      "metadata": {
        "id": "F_gfOxWabtxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):     \n",
        "    with tf.GradientTape() as tape: \n",
        "        pred = model(X[i])\n",
        "        print('Prediction is', pred.numpy(), 'True is', y[i])\n",
        "\n",
        "        loss_value = MSE(y[i], pred)\n",
        "        print('Loss is', loss_value.numpy())\n",
        "\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights) \n",
        "        print('Grad are', grads / loss_value)\n",
        "        print('_' * 40)"
      ],
      "metadata": {
        "id": "WsZ4uuGlbtxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leaky ReLU\n",
        "\n",
        "\n",
        "$$A(x) = max(0.1x, x)$$\n",
        "\n",
        "\n",
        "Например, для решения проблемы нулевого градиента имеет смысл заменить горизонтальную часть функции на линейную.\n",
        "\n",
        "Получаем слегка отклоненную линию от горизонтального положения. Суть в том, чтобы сделать градиент неравным нулю и постепенно восстанавливать его во время тренировки."
      ],
      "metadata": {
        "id": "d5VYGAW3UOWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.activations import leaky_relu\n",
        "\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = leaky_relu(x, alpha=0.1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('leaky ReLU')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "D4uwAvENUPiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Производная leaky ReLU"
      ],
      "metadata": {
        "id": "8o4KFtBDUPiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu_grad(x):\n",
        "    return np.where(x >= 0, 1, 0.1)"
      ],
      "metadata": {
        "id": "pukyEl7eUPiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "y = leaky_relu(x, alpha=0.1)\n",
        "\n",
        "grad_y = leaky_relu_grad(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.plot(x, grad_y, c='g', label='grad')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('leaky ReLU')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "vHSDyja1UPiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELU\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1bCygDVQiwQP7lu-m0gyQy6EjyCSa1YA9' width=200>\n",
        "\n",
        "Или же можем нелинейно отклонять линию от горизонтального положения, а более плавно.\n",
        "\n",
        "Но суть остается такая же - делаем градиент ненулевым."
      ],
      "metadata": {
        "id": "Muhv9htSVH8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.activations import elu\n",
        "\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "alpha = 0.3\n",
        "y = elu(x, alpha=alpha)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('elu')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "SCcXV91zVH8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Производная ELU"
      ],
      "metadata": {
        "id": "UKOkKcxMVH8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def elu_grad(x):\n",
        "    return np.where(x >= 0, 1, elu(x, alpha=alpha) + alpha)"
      ],
      "metadata": {
        "id": "UIBBUBz5VH8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "y = elu(x, alpha=alpha)\n",
        "\n",
        "grad_y = elu_grad(x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, c='b', label='activation')\n",
        "plt.plot(x, grad_y, c='g', label='grad')\n",
        "plt.ylabel('output')\n",
        "plt.xlabel('input')\n",
        "plt.title('ELU')\n",
        "plt.legend()\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "ooi_ijtwVH8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Как выбрать функцию активации?\n",
        "\n",
        "Как и во всем глубоком обучении, здесь нет очевидного ответа. Выбирайте ту функцию, которая ведет к более быстрому обучению.\n",
        "\n",
        "Есть только однозначный ответ касаемо функции активации на последнем слое. Если у вас задача классификации, то самый лучший кандидат - sigmoid, т.к. выдает по сути вероятность быть одним классом или другим.\n",
        "\n",
        "А если задача регрессии, то выбирайте ReLU - если у вас не может быть отрицательного целевого значения или же линейную активацию.\n",
        "\n",
        "Ну и конечно же, можно использоваться кастомные функции активации."
      ],
      "metadata": {
        "id": "Rll0DgezUbBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Кастомная функция активации"
      ],
      "metadata": {
        "id": "Omm9ekUeSYv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на проблемы очень больших градиентов."
      ],
      "metadata": {
        "id": "oIJFZQ2EqMmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(10, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "T0_mVb06hyFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для этого поставим очень большие веса.\n",
        "\n",
        "Такие веса могут получится, если градиент будет всегда большой."
      ],
      "metadata": {
        "id": "XZ3bxu8IqPPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(9)\n",
        "\n",
        "new_weights = []\n",
        "#пройдемся по текущим весам get_weights()\n",
        "for weights in model.get_weights():\n",
        "    if len(weights.shape) == 2:\n",
        "        new_weights.append(np.random.randn(weights.shape[0], weights.shape[1]) * 100) #будет добавлять случайные веса из норм. распред. нужной размерности •100\n",
        "    else:\n",
        "        new_weights.append(np.random.randn(weights.shape[0]) * 100) #bias (1 размерноть) – свобоные веса\n",
        "\n",
        "model.set_weights(new_weights)\n",
        "\n",
        "model.get_weights()[:2]"
      ],
      "metadata": {
        "id": "Zv_yIojviLbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.01)\n",
        "loss_fn = CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "sNBF9dz9hyFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А видим, что уже ошибка посчитаться не может, что-то пошло не так."
      ],
      "metadata": {
        "id": "G_RIQPxGqanS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grad_log = []\n",
        "\n",
        "epochs = 2 \n",
        "\n",
        "for epoch in range(epochs): \n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): \n",
        "        \n",
        "        with tf.GradientTape() as tape: \n",
        "            preds = model(x_batch_train) \n",
        "\n",
        "            loss_value = loss_fn(y_batch_train, preds)\n",
        "\n",
        "            grads = tape.gradient(loss_value, model.trainable_weights) \n",
        "            g_g = []\n",
        "\n",
        "            for g_s in grads:\n",
        "                if len(g_s.numpy().shape) == 2:\n",
        "                    g_g.append(g_s.numpy()[0, 0]) \n",
        "\n",
        "        grad_log.append(g_g)\n",
        "\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}', end='. ')\n",
        "            print(f'Step {step}. Train loss: {loss_value}') "
      ],
      "metadata": {
        "id": "vfVz8QlyhyFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А веса тоже посчитаться не могут, они вышли за разрешенные значения float32."
      ],
      "metadata": {
        "id": "p4w0zR3jqgdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_weights()[:2]"
      ],
      "metadata": {
        "id": "4VfBiS0ligQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Но можем изменить фукнцию активации ReLU.\n",
        "\n",
        "Раньше у неё не было ограничений в большую сторону, а значит выходные значения активации могли быть очень большими.\n",
        "\n",
        "Но теперь поставим ограничения в 6, чтобы как-то утихомирить наши сигнала, а значит и веса."
      ],
      "metadata": {
        "id": "_IlhMgQzqnds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def my_relu(x):\n",
        "    return K.relu(x, max_value=6)"
      ],
      "metadata": {
        "id": "5j5xY3DSiv1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_relu([10, 0, -2, 7])"
      ],
      "metadata": {
        "id": "rN9rFAGtjQwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation=my_relu),\n",
        "    Dense(64, activation=my_relu),\n",
        "    Dense(32, activation=my_relu),\n",
        "    Dense(16, activation=my_relu),\n",
        "    Dense(10, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "oX8E6Xjbiuyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.set_weights(new_weights)"
      ],
      "metadata": {
        "id": "dti7GO6riuyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.01)\n",
        "loss_fn = CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "b3YNx6Ybiuym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение идёт, nan'ов не видно."
      ],
      "metadata": {
        "id": "qydopanXq305"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grad_log = []\n",
        "\n",
        "epochs = 2 \n",
        "\n",
        "for epoch in range(epochs): \n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): \n",
        "        \n",
        "        with tf.GradientTape() as tape: \n",
        "            preds = model(x_batch_train) \n",
        "\n",
        "            loss_value = loss_fn(y_batch_train, preds)\n",
        "\n",
        "            grads = tape.gradient(loss_value, model.trainable_weights) \n",
        "            g_g = []\n",
        "\n",
        "            for g_s in grads:\n",
        "                if len(g_s.numpy().shape) == 2:\n",
        "                    g_g.append(g_s.numpy()[0, 0]) \n",
        "\n",
        "        grad_log.append(g_g)\n",
        "\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}', end='. ')\n",
        "            print(f'Step {step}. Train loss: {loss_value}') "
      ],
      "metadata": {
        "id": "8z64OHg0iuym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_weights()[:2]"
      ],
      "metadata": {
        "id": "gnjR7oQRiuym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Сегодня обсудили:\n",
        "1. Какие есть функции активации нейронов (Sigmoid, ReLU, ELU, Leaky ReLU...)\n",
        "2. Плюсы каждой функции активации\n",
        "3. Минусы каждой функции активации\n",
        "4. Как написать свою функцию активации\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<th>\n",
        "Функция активации\n",
        "</th>\n",
        "\n",
        "<th>\n",
        "Плюсы\n",
        "</th>\n",
        "\n",
        "<th>\n",
        "Минусы\n",
        "</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>\n",
        "linear\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Меньше времени на вычисления\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Линейная функция\n",
        "        2. Нефиксированный диапазон значений функции — [-∞,+∞]\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "\n",
        "<tr>\n",
        "<td>\n",
        "Sigmoid\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Нелинейная функция\n",
        "        2. В диапазоне значений сигнала от -2.5 до 2.5 значения активации меняются очень быстро\n",
        "        3. Фиксированный диапазон значений функции — [0,1]\n",
        "\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Затухают градиенты \n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>\n",
        "Tanh\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Нелинейная функция\n",
        "        2. В диапазоне значений сигнала от -2.5 до 2.5 значения активации меняются очень быстро\n",
        "        3. Фиксированный диапазон значений функции — [-1,1]\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Затухают градиенты \n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>\n",
        "ReLU\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Меньше времени на вычисления\n",
        "        2. Меньше вес у сети\n",
        "        3. Нет проблемы затухающих градиентов \n",
        "    \n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Нулевые градиенты у сигналов, которые меньше 0\n",
        "        2. Нефиксированный диапазон значений функции\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>\n",
        "Leaky ReLU\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Нет проблемы нулевых градиентов\n",
        "\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Нужно подбирать значение alpha\n",
        "        2. Нефиксированный диапазон значений функции\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>\n",
        "ELU\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Нет проблемы нулевых градиентов\n",
        "        2. Имеются негативные выходы, что помогает нейронке направлять веса в нужное напрвление\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "        1. Больше времени на вычисления\n",
        "        2. Нефиксированный диапазон значений функции\n",
        "        3. Нужно подбирать значение alpha\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "ztwSPvVzvNlU"
      }
    }
  ]
}